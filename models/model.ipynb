{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-02T01:02:58.811027Z",
     "start_time": "2025-11-02T01:02:52.065478Z"
    }
   },
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # For saving the model and preprocessor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Import SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Use this pipeline to chain preprocessor and model\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "CLEANED_DATA_PATH = 'cleaned_diabetic_data.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "    print(f\"Loaded cleaned data. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{CLEANED_DATA_PATH}' not found. Please check the file path.\")\n",
    "    # Stop execution if file isn't found\n",
    "    raise\n",
    "\n",
    "TARGET_COLUMN = 'target'\n",
    "\n",
    "COLS_TO_DROP_FOR_MODELING = ['readmitted', 'diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "try:\n",
    "    y = df[TARGET_COLUMN]\n",
    "    X = df.drop([TARGET_COLUMN] + COLS_TO_DROP_FOR_MODELING, axis=1)\n",
    "    print(\"Features (X) and Target (y) are defined.\")\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A required column is missing. {e}\")\n",
    "    print(\"Please ensure your cleaned data contains 'target' and the original 'diag' columns.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"\\nIdentified {len(numerical_cols)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_cols)} categorical features.\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# We split FIRST, before applying SMOTE, to keep our test set realistic.\n",
    "# stratify=y is CRITICAL for imbalanced datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,    # 20% for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y        # Ensures same class balance in train and test\n",
    ")\n",
    "\n",
    "print(f\"\\nData split into training and testing sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(f\"Class 1 (Readmitted) in y_train: {y_train.sum()} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Class 1 (Readmitted) in y_test:  {y_test.sum()} ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# Define the Random Forest model\n",
    "# n_jobs=-1 uses all available CPU cores for speed\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create an imblearn pipeline that:\n",
    "# 1. Applies the 'preprocessor' (scaling and one-hot encoding)\n",
    "# 2. Applies 'smote' to the training data (and *only* the training data)\n",
    "# 3. Trains the 'rf_model'\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "print(\"\\nStarting Model Training (with SMOTE)\")\n",
    "# Fit the entire pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model Training Complete\")\n",
    "\n",
    "print(\"\\nModel Evaluation (on Unseen Test Set)\")\n",
    "\n",
    "# Make predictions on the original, non-resampled test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] # Probabilities for class 1\n",
    "\n",
    "# Print the classification report\n",
    "# This is the key evaluation table\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Readmitted (0)', 'Readmitted (1)']))\n",
    "\n",
    "# Print the AUC-ROC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix\")\n",
    "# This shows the raw numbers for 'Social Welfare' analysis\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"                 Predicted 0   |   Predicted 1\")\n",
    "print(f\"Actual 0:    {cm[0,0]:>10}    |    {cm[0,1]:>10}  (False Positives)\")\n",
    "print(f\"Actual 1:    {cm[1,0]:>10}    |    {cm[1,1]:>10}  (True Positives)\")\n",
    "print(f\"\\nKey Metric (Recall): The model correctly identified {cm[1,1]} out of {cm[1,0] + cm[1,1]} actual readmissions.\")\n",
    "\n",
    "# Save the entire trained pipeline (preprocessor + SMOTE + model)\n",
    "pipeline_filename = 'rf_smote_pipeline.joblib'\n",
    "joblib.dump(pipeline, pipeline_filename)\n",
    "print(f\"\\nSuccessfully saved the full pipeline to '{pipeline_filename}'\")\n",
    "\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "print(\"Successfully saved 'X_test.csv' and 'y_test.csv' for the visualization notebook.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Loaded cleaned data. Shape: (67113, 46)\n",
      "Features (X) and Target (y) are defined.\n",
      "X shape: (67113, 41), y shape: (67113,)\n",
      "\n",
      "Identified 11 numerical features.\n",
      "Identified 30 categorical features.\n",
      "\n",
      "Data split into training and testing sets:\n",
      "X_train shape: (53690, 41), y_train shape: (53690,)\n",
      "X_test shape: (13423, 41), y_test shape: (13423,)\n",
      "Class 1 (Readmitted) in y_train: 4888 (9.10%)\n",
      "Class 1 (Readmitted) in y_test:  1222 (9.10%)\n",
      "\n",
      "Starting Model Training (with SMOTE)\n",
      "Model Training Complete\n",
      "\n",
      "Model Evaluation (on Unseen Test Set)\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Not Readmitted (0)       0.91      1.00      0.95     12201\n",
      "    Readmitted (1)       0.10      0.00      0.00      1222\n",
      "\n",
      "          accuracy                           0.91     13423\n",
      "         macro avg       0.51      0.50      0.48     13423\n",
      "      weighted avg       0.84      0.91      0.87     13423\n",
      "\n",
      "AUC-ROC Score: 0.6068\n",
      "\n",
      "Confusion Matrix\n",
      "                 Predicted 0   |   Predicted 1\n",
      "Actual 0:         12175    |            26  (False Positives)\n",
      "Actual 1:          1219    |             3  (True Positives)\n",
      "\n",
      "Key Metric (Recall): The model correctly identified 3 out of 1222 actual readmissions.\n",
      "\n",
      "Successfully saved the full pipeline to 'rf_smote_pipeline.joblib'\n",
      "Successfully saved 'X_test.csv' and 'y_test.csv' for the visualization notebook.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T01:05:01.671326Z",
     "start_time": "2025-11-02T01:04:28.118313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "CLEANED_DATA_PATH = 'cleaned_diabetic_data.csv'\n",
    "try:\n",
    "    df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "    print(f\"Loaded cleaned data. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{CLEANED_DATA_PATH}' not found.\")\n",
    "    raise\n",
    "\n",
    "TARGET_COLUMN = 'target'\n",
    "COLS_TO_DROP_FOR_MODELING = ['readmitted', 'diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "try:\n",
    "    y = df[TARGET_COLUMN]\n",
    "    X = df.drop([TARGET_COLUMN] + COLS_TO_DROP_FOR_MODELING, axis=1)\n",
    "    print(f\"Features (X) and Target (y) are defined. X shape: {X.shape}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A required column is missing. {e}\")\n",
    "    raise\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Identified {len(numerical_cols)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_cols)} categorical features.\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "print(f\"Data split into training and testing sets.\")\n",
    "\n",
    "# standard sklearn pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"\\nModel Pipeline Created\")\n",
    "\n",
    "\n",
    "# Define the \"grid\" of parameters to search\n",
    "# We'll test a few key parameters\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 150],            # Number of trees\n",
    "    'model__max_depth': [10, 20],                 # Max depth of each tree\n",
    "    'model__min_samples_leaf': [5, 10],           # Min samples at a leaf node (helps prevent overfitting)\n",
    "    'model__class_weight': ['balanced', 'balanced_subsample'] # This is the KEY change\n",
    "}\n",
    "\n",
    "# Set up the Grid Search\n",
    "# IMPORTANT: scoring='recall' tells the search to find the\n",
    "# model with the highest RECALL for the positive class.\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,        # Use all cores\n",
    "    verbose=2         # Show progress\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Hyperparameter Tuning (GridSearchCV)\")\n",
    "print(\"This will take several minutes\")\n",
    "# Fit the Grid Search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tuning Complete\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best recall score (on validation data): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nEvaluation of BEST Model (on Unseen Test Set)\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1] # Probabilities for class 1\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Readmitted (0)', 'Readmitted (1)']))\n",
    "\n",
    "# Print the AUC-ROC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"                 Predicted 0   |   Predicted 1\")\n",
    "print(f\"Actual 0:    {cm[0,0]:>10}    |    {cm[0,1]:>10}  (False Positives)\")\n",
    "print(f\"Actual 1:    {cm[1,0]:>10}    |    {cm[1,1]:>10}  (True Positives)\")\n",
    "\n",
    "recall = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(f\"\\nKey Metric (Recall): The model correctly identified {cm[1,1]} out of {cm[1,0] + cm[1,1]} actual readmissions. (Recall = {recall:.2%})\")\n",
    "\n",
    "# Save the entire pipeline\n",
    "pipeline_filename = 'rf_tuned_pipeline.joblib'\n",
    "joblib.dump(best_model, pipeline_filename)\n",
    "print(f\"\\nSuccessfully saved the tuned pipeline to '{pipeline_filename}'\")\n",
    "\n",
    "# Save the test sets (no change here)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "print(\"Successfully saved 'X_test.csv' and 'y_test.csv' for the visualization notebook.\")"
   ],
   "id": "b76cba990bfd9f9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Loaded cleaned data. Shape: (67113, 46)\n",
      "Features (X) and Target (y) are defined. X shape: (67113, 41)\n",
      "Identified 11 numerical features.\n",
      "Identified 30 categorical features.\n",
      "Data split into training and testing sets.\n",
      "\n",
      "Model Pipeline Created\n",
      "\n",
      "Starting Hyperparameter Tuning (GridSearchCV)\n",
      "This will take several minutes\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Tuning Complete\n",
      "Best parameters found: {'model__class_weight': 'balanced_subsample', 'model__max_depth': 10, 'model__min_samples_leaf': 10, 'model__n_estimators': 150}\n",
      "Best recall score (on validation data): 0.4673\n",
      "\n",
      "Evaluation of BEST Model (on Unseen Test Set)\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Not Readmitted (0)       0.93      0.69      0.79     12201\n",
      "    Readmitted (1)       0.14      0.49      0.21      1222\n",
      "\n",
      "          accuracy                           0.67     13423\n",
      "         macro avg       0.53      0.59      0.50     13423\n",
      "      weighted avg       0.86      0.67      0.74     13423\n",
      "\n",
      "AUC-ROC Score: 0.6358\n",
      "\n",
      "Confusion Matrix\n",
      "                 Predicted 0   |   Predicted 1\n",
      "Actual 0:          8462    |          3739  (False Positives)\n",
      "Actual 1:           628    |           594  (True Positives)\n",
      "\n",
      "Key Metric (Recall): The model correctly identified 594 out of 1222 actual readmissions. (Recall = 48.61%)\n",
      "\n",
      "Successfully saved the tuned pipeline to 'rf_tuned_pipeline.joblib'\n",
      "Successfully saved 'X_test.csv' and 'y_test.csv' for the visualization notebook.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c1878fa91cf9ff5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
