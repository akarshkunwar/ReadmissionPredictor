{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T18:42:09.122489Z",
     "start_time": "2025-12-11T18:42:03.141881Z"
    }
   },
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Import SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "CLEANED_DATA_PATH = 'cleaned_diabetic_data.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "    print(f\"Loaded cleaned data. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{CLEANED_DATA_PATH}' not found. Please check the file path.\")\n",
    "    # Stop execution if file isn't found\n",
    "    raise\n",
    "\n",
    "TARGET_COLUMN = 'target'\n",
    "\n",
    "COLS_TO_DROP_FOR_MODELING = ['readmitted', 'diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "try:\n",
    "    y = df[TARGET_COLUMN]\n",
    "    X = df.drop([TARGET_COLUMN] + COLS_TO_DROP_FOR_MODELING, axis=1)\n",
    "    print(\"Features (X) and Target (y) are defined.\")\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A required column is missing. {e}\")\n",
    "    print(\"Please ensure your cleaned data contains 'target' and the original 'diag' columns.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"\\nIdentified {len(numerical_cols)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_cols)} categorical features.\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "#preprocessor pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# split before oversampling to preserve test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nData split into training and testing sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(f\"Class 1 (Readmitted) in y_train: {y_train.sum()} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Class 1 (Readmitted) in y_test:  {y_test.sum()} ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# initialize random forest classifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# imblearn pipeline with SMOTE\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "print(\"\\nStarting Model Training (with SMOTE)\")\n",
    "# fit the entire pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model Training Complete\")\n",
    "\n",
    "print(\"\\nModel Evaluation (on Unseen Test Set)\")\n",
    "\n",
    "# make predictions on the original, non-resampled test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] # Probabilities for class 1\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Readmitted (0)', 'Readmitted (1)']))\n",
    "\n",
    "# print the AUC-ROC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "# confusion matrix\n",
    "print(\"\\nConfusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"                 Predicted 0   |   Predicted 1\")\n",
    "print(f\"Actual 0:    {cm[0,0]:>10}    |    {cm[0,1]:>10}  (False Positives)\")\n",
    "print(f\"Actual 1:    {cm[1,0]:>10}    |    {cm[1,1]:>10}  (True Positives)\")\n",
    "print(f\"\\nKey Metric (Recall): The model correctly identified {cm[1,1]} out of {cm[1,0] + cm[1,1]} actual readmissions.\")\n",
    "\n",
    "# save trained pipeline\n",
    "pipeline_filename = 'rf_smote_pipeline.joblib'\n",
    "joblib.dump(pipeline, pipeline_filename)\n",
    "print(f\"\\nSuccessfully saved the full pipeline to '{pipeline_filename}'\")\n",
    "\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "print(\"Successfully saved 'X_test.csv' and 'y_test.csv' for the visualization notebook.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Loaded cleaned data. Shape: (67113, 46)\n",
      "Features (X) and Target (y) are defined.\n",
      "X shape: (67113, 41), y shape: (67113,)\n",
      "\n",
      "Identified 11 numerical features.\n",
      "Identified 30 categorical features.\n",
      "\n",
      "Data split into training and testing sets:\n",
      "X_train shape: (53690, 41), y_train shape: (53690,)\n",
      "X_test shape: (13423, 41), y_test shape: (13423,)\n",
      "Class 1 (Readmitted) in y_train: 4888 (9.10%)\n",
      "Class 1 (Readmitted) in y_test:  1222 (9.10%)\n",
      "\n",
      "Starting Model Training (with SMOTE)\n",
      "Model Training Complete\n",
      "\n",
      "Model Evaluation (on Unseen Test Set)\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Not Readmitted (0)       0.91      1.00      0.95     12201\n",
      "    Readmitted (1)       0.10      0.00      0.00      1222\n",
      "\n",
      "          accuracy                           0.91     13423\n",
      "         macro avg       0.51      0.50      0.48     13423\n",
      "      weighted avg       0.84      0.91      0.87     13423\n",
      "\n",
      "AUC-ROC Score: 0.6068\n",
      "\n",
      "Confusion Matrix\n",
      "                 Predicted 0   |   Predicted 1\n",
      "Actual 0:         12175    |            26  (False Positives)\n",
      "Actual 1:          1219    |             3  (True Positives)\n",
      "\n",
      "Key Metric (Recall): The model correctly identified 3 out of 1222 actual readmissions.\n",
      "\n",
      "Successfully saved the full pipeline to 'rf_smote_pipeline.joblib'\n",
      "Successfully saved 'X_test.csv' and 'y_test.csv' for the visualization notebook.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:43:09.197865Z",
     "start_time": "2025-12-11T18:42:32.294505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "CLEANED_DATA_PATH = 'cleaned_diabetic_data.csv'\n",
    "try:\n",
    "    df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "    print(f\"Loaded cleaned data. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{CLEANED_DATA_PATH}' not found.\")\n",
    "    raise\n",
    "\n",
    "TARGET_COLUMN = 'target'\n",
    "COLS_TO_DROP_FOR_MODELING = ['readmitted', 'diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "try:\n",
    "    y = df[TARGET_COLUMN]\n",
    "    X = df.drop([TARGET_COLUMN] + COLS_TO_DROP_FOR_MODELING, axis=1)\n",
    "    print(f\"Features (X) and Target (y) are defined. X shape: {X.shape}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A required column is missing. {e}\")\n",
    "    raise\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Identified {len(numerical_cols)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_cols)} categorical features.\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "print(f\"Data split into training and testing sets.\")\n",
    "\n",
    "# standard sklearn pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"\\nModel Pipeline Created\")\n",
    "\n",
    "\n",
    "# define the grid of parameters to search\n",
    "# testing a few key parameters\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 150],\n",
    "    'model__max_depth': [10, 20],\n",
    "    'model__min_samples_leaf': [5, 10],\n",
    "    'model__class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# initialize gridsearch\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Hyperparameter Tuning (GridSearchCV)\")\n",
    "print(\"This will take several minutes\")\n",
    "\n",
    "# fit the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tuning Complete\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best recall score (on validation data): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nEvaluation of best Model (on Unseen Test Set)\")\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1] # Probabilities for class 1\n",
    "\n",
    "# print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Readmitted (0)', 'Readmitted (1)']))\n",
    "\n",
    "# print the AUC-ROC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"                 Predicted 0   |   Predicted 1\")\n",
    "print(f\"Actual 0:    {cm[0,0]:>10}    |    {cm[0,1]:>10}  (False Positives)\")\n",
    "print(f\"Actual 1:    {cm[1,0]:>10}    |    {cm[1,1]:>10}  (True Positives)\")\n",
    "\n",
    "recall = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(f\"\\nKey Metric (Recall): The model correctly identified {cm[1,1]} out of {cm[1,0] + cm[1,1]}\"\n",
    "      f\" actual readmissions. (Recall = {recall:.2%})\")\n",
    "\n",
    "# save the entire pipeline\n",
    "pipeline_filename = 'rf_tuned_pipeline.joblib'\n",
    "joblib.dump(best_model, pipeline_filename)\n",
    "print(f\"\\nSuccessfully saved the tuned pipeline to '{pipeline_filename}'\")\n",
    "\n",
    "# save the test seta\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "#print(\"Successfully saved 'X_test.csv' and 'y_test.csv' for the visualization notebook.\")"
   ],
   "id": "b76cba990bfd9f9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Loaded cleaned data. Shape: (67113, 46)\n",
      "Features (X) and Target (y) are defined. X shape: (67113, 41)\n",
      "Identified 11 numerical features.\n",
      "Identified 30 categorical features.\n",
      "Data split into training and testing sets.\n",
      "\n",
      "Model Pipeline Created\n",
      "\n",
      "Starting Hyperparameter Tuning (GridSearchCV)\n",
      "This will take several minutes\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Tuning Complete\n",
      "Best parameters found: {'model__class_weight': 'balanced_subsample', 'model__max_depth': 10, 'model__min_samples_leaf': 10, 'model__n_estimators': 150}\n",
      "Best recall score (on validation data): 0.4673\n",
      "\n",
      "Evaluation of best Model (on Unseen Test Set)\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Not Readmitted (0)       0.93      0.69      0.79     12201\n",
      "    Readmitted (1)       0.14      0.49      0.21      1222\n",
      "\n",
      "          accuracy                           0.67     13423\n",
      "         macro avg       0.53      0.59      0.50     13423\n",
      "      weighted avg       0.86      0.67      0.74     13423\n",
      "\n",
      "AUC-ROC Score: 0.6358\n",
      "\n",
      "Confusion Matrix\n",
      "                 Predicted 0   |   Predicted 1\n",
      "Actual 0:          8462    |          3739  (False Positives)\n",
      "Actual 1:           628    |           594  (True Positives)\n",
      "\n",
      "Key Metric (Recall): The model correctly identified 594 out of 1222 actual readmissions. (Recall = 48.61%)\n",
      "\n",
      "Successfully saved the tuned pipeline to 'rf_tuned_pipeline.joblib'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:43:27.367542Z",
     "start_time": "2025-12-11T18:43:09.208151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# print(\"Libraries imported successfully.\")\n",
    "\n",
    "# loading data\n",
    "df = pd.read_csv('cleaned_diabetic_data.csv')\n",
    "y = df['target']\n",
    "X = df.drop(['target', 'readmitted', 'diag_1', 'diag_2', 'diag_3'], axis=1)\n",
    "\n",
    "# preprocessing\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# calculate weights for imbalance\n",
    "# formula: sum(negative instances) / sum(positive instances)\n",
    "# this tells XGBoost to treat the minority class with x times more weight\n",
    "scale_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"Calculated scale_pos_weight: {scale_weight:.2f}\")\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# build XGBoost pipeline\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=scale_weight, # handles the imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb_model)\n",
    "])\n",
    "\n",
    "# hyperparameter tuning\n",
    "# key hyperparameter selection\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"\\nStarting XGBoost Grid Search (F1 Score)\")\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# model evaluation\n",
    "print(\"\\nXGBoost Evaluation\")\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\nConfusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"                 Predicted 0   |   Predicted 1\")\n",
    "print(f\"Actual 0:    {cm[0,0]:>10}    |    {cm[0,1]:>10}  (False Positives)\")\n",
    "print(f\"Actual 1:    {cm[1,0]:>10}    |    {cm[1,1]:>10}  (True Positives)\")\n",
    "\n",
    "recall = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(f\"\\nKey Metric (Recall): The model correctly identified {cm[1,1]} out of {cm[1,0] + cm[1,1]}\"\n",
    "      f\" actual readmissions. (Recall = {recall:.2%})\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_xgb, 'xgb_tuned_pipeline.joblib')\n",
    "print(\"XGBoost model saved.\")\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# find the exact threshold for 70% recall\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# calculate precision and recall for all possible thresholds\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# we want the threshold where recall is closest to 0.70\n",
    "target_recall = 0.70\n",
    "\n",
    "# find the index of the recall value closest to 0.70\n",
    "optimal_idx = np.argmin(np.abs(recalls - target_recall))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Threshold Tuning Results\")\n",
    "print(f\"To achieve ~{target_recall*100}% Recall, the threshold moves from 0.50 to: {optimal_threshold:.4f}\")\n",
    "\n",
    "# applying new threshold\n",
    "y_pred_optimized = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\n--- Optimized Classification Report (Threshold: {:.4f}) ---\".format(optimal_threshold))\n",
    "print(classification_report(y_test, y_pred_optimized, target_names=['Not Readmitted (0)', 'Readmitted (1)']))\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\nConfusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred_optimized)\n",
    "print(\"                 Predicted 0   |   Predicted 1\")\n",
    "print(f\"Actual 0:    {cm[0,0]:>10}    |    {cm[0,1]:>10}  (False Positives)\")\n",
    "print(f\"Actual 1:    {cm[1,0]:>10}    |    {cm[1,1]:>10}  (True Positives)\")\n",
    "\n",
    "recall = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(f\"\\nKey Metric (Recall): The model correctly identified {cm[1,1]} out of {cm[1,0] + cm[1,1]}\"\n",
    "      f\" actual readmissions. (Recall = {recall:.2%})\")"
   ],
   "id": "9c1878fa91cf9ff5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated scale_pos_weight: 9.98\n",
      "\n",
      "Starting XGBoost Grid Search (F1 Score)\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best params: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 200, 'model__subsample': 0.8}\n",
      "\n",
      "XGBoost Evaluation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.67      0.78     12201\n",
      "           1       0.14      0.54      0.22      1222\n",
      "\n",
      "    accuracy                           0.65     13423\n",
      "   macro avg       0.54      0.60      0.50     13423\n",
      "weighted avg       0.86      0.65      0.73     13423\n",
      "\n",
      "AUC-ROC Score: 0.6385\n",
      "\n",
      "Confusion Matrix\n",
      "                 Predicted 0   |   Predicted 1\n",
      "Actual 0:          8117    |          4084  (False Positives)\n",
      "Actual 1:           563    |           659  (True Positives)\n",
      "\n",
      "Key Metric (Recall): The model correctly identified 659 out of 1222 actual readmissions. (Recall = 53.93%)\n",
      "XGBoost model saved.\n",
      "Threshold Tuning Results\n",
      "To achieve ~70.0% Recall, the threshold moves from 0.50 to: 0.4396\n",
      "\n",
      "--- Optimized Classification Report (Threshold: 0.4396) ---\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Not Readmitted (0)       0.94      0.48      0.64     12201\n",
      "    Readmitted (1)       0.12      0.70      0.20      1222\n",
      "\n",
      "          accuracy                           0.50     13423\n",
      "         macro avg       0.53      0.59      0.42     13423\n",
      "      weighted avg       0.87      0.50      0.60     13423\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "                 Predicted 0   |   Predicted 1\n",
      "Actual 0:          5873    |          6328  (False Positives)\n",
      "Actual 1:           367    |           855  (True Positives)\n",
      "\n",
      "Key Metric (Recall): The model correctly identified 855 out of 1222 actual readmissions. (Recall = 69.97%)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:43:39.304868Z",
     "start_time": "2025-12-11T18:43:39.217090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# optimal threshold search\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# using old probabilities\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calculate precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# find the threshold that gives us at least 70% recall\n",
    "target_recall = 0.70\n",
    "idx = np.argmax(recalls <= target_recall)\n",
    "optimal_threshold = thresholds[idx]\n",
    "\n",
    "print(f\"To achieve {target_recall*100}% Recall, set threshold to: {optimal_threshold:.4f}\")\n",
    "\n",
    "# apply this new threshold\n",
    "y_pred_optimized = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# see the new result\n",
    "print(\"\\n--- Optimized Threshold Results ---\")\n",
    "print(classification_report(y_test, y_pred_optimized))\n",
    "\n",
    "# confusion matrix\n",
    "print(\"\\nConfusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"                 Predicted 0   |   Predicted 1\")\n",
    "print(f\"Actual 0:    {cm[0,0]:>10}    |    {cm[0,1]:>10}  (False Positives)\")\n",
    "print(f\"Actual 1:    {cm[1,0]:>10}    |    {cm[1,1]:>10}  (True Positives)\")\n",
    "\n",
    "recall = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "print(f\"\\nKey Metric (Recall): The model correctly identified {cm[1,1]} out of {cm[1,0] + cm[1,1]}\"\n",
    "      f\" actual readmissions. (Recall = {recall:.2%})\")"
   ],
   "id": "8a45a7e65c1e2ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To achieve 70.0% Recall, set threshold to: 0.4396\n",
      "\n",
      "--- Optimized Threshold Results ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.48      0.64     12201\n",
      "           1       0.12      0.70      0.20      1222\n",
      "\n",
      "    accuracy                           0.50     13423\n",
      "   macro avg       0.53      0.59      0.42     13423\n",
      "weighted avg       0.87      0.50      0.60     13423\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "                 Predicted 0   |   Predicted 1\n",
      "Actual 0:          8117    |          4084  (False Positives)\n",
      "Actual 1:           563    |           659  (True Positives)\n",
      "\n",
      "Key Metric (Recall): The model correctly identified 659 out of 1222 actual readmissions. (Recall = 53.93%)\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
